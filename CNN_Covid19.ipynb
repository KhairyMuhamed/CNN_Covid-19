{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset link\n",
    "## https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Convolution2D \n",
    "from keras.layers import MaxPooling2D \n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation, Dense ,Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing CNN\n",
    "model = Sequential()\n",
    "\n",
    "#Step 1 - Convolution\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(64,64,3), padding='same'))\n",
    "\n",
    "#step 2 - MaxPooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Adding another layer to improve the accuracy\n",
    "model.add(Convolution2D(32, (3, 3),  activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Adding another layer to improve the accuracy\n",
    "model.add(Convolution2D(32, (3, 3),  activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Adding another layer to improve the accuracy\n",
    "model.add(Convolution2D(64, (3, 3),  activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Adding another layer to improve the accuracy\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Add an second Convolution layer \n",
    "model.add(Convolution2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#step 3 - Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "#step 4 - Full Connection\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 115,969\n",
      "Trainable params: 115,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the CNN\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n",
      "Epoch 1/200\n",
      "82/82 [==============================] - ETA: 0s - loss: 0.5960 - accuracy: 0.7324WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 82 batches). You may need to use the repeat() function when building your dataset.\n",
      "82/82 [==============================] - 101s 1s/step - loss: 0.5960 - accuracy: 0.7324 - val_loss: 0.7574 - val_accuracy: 0.6250\n",
      "Epoch 2/200\n",
      "82/82 [==============================] - 111s 1s/step - loss: 0.3731 - accuracy: 0.8294\n",
      "Epoch 3/200\n",
      "82/82 [==============================] - 111s 1s/step - loss: 0.2193 - accuracy: 0.9147\n",
      "Epoch 4/200\n",
      "82/82 [==============================] - 117s 1s/step - loss: 0.1944 - accuracy: 0.9289\n",
      "Epoch 5/200\n",
      "82/82 [==============================] - 111s 1s/step - loss: 0.1820 - accuracy: 0.9258\n",
      "Epoch 6/200\n",
      "82/82 [==============================] - 124s 2s/step - loss: 0.1646 - accuracy: 0.9379\n",
      "Epoch 7/200\n",
      "82/82 [==============================] - 108s 1s/step - loss: 0.1551 - accuracy: 0.9421\n",
      "Epoch 8/200\n",
      "82/82 [==============================] - 133s 2s/step - loss: 0.1391 - accuracy: 0.9486\n",
      "Epoch 9/200\n",
      "82/82 [==============================] - 140s 2s/step - loss: 0.1658 - accuracy: 0.9406\n",
      "Epoch 10/200\n",
      "82/82 [==============================] - 119s 1s/step - loss: 0.1362 - accuracy: 0.9511\n",
      "Epoch 11/200\n",
      "82/82 [==============================] - 90s 1s/step - loss: 0.1322 - accuracy: 0.9546\n",
      "Epoch 12/200\n",
      "82/82 [==============================] - 88s 1s/step - loss: 0.1293 - accuracy: 0.9544\n",
      "Epoch 13/200\n",
      "82/82 [==============================] - 84s 1s/step - loss: 0.1315 - accuracy: 0.9519\n",
      "Epoch 14/200\n",
      "82/82 [==============================] - 84s 1s/step - loss: 0.1274 - accuracy: 0.9515\n",
      "Epoch 15/200\n",
      "82/82 [==============================] - 85s 1s/step - loss: 0.1263 - accuracy: 0.9528\n",
      "Epoch 16/200\n",
      "82/82 [==============================] - 82s 995ms/step - loss: 0.1165 - accuracy: 0.9559\n",
      "Epoch 17/200\n",
      "82/82 [==============================] - 81s 988ms/step - loss: 0.1153 - accuracy: 0.9532\n",
      "Epoch 18/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.1047 - accuracy: 0.9622\n",
      "Epoch 19/200\n",
      "82/82 [==============================] - 82s 999ms/step - loss: 0.1121 - accuracy: 0.9584\n",
      "Epoch 20/200\n",
      "82/82 [==============================] - 81s 986ms/step - loss: 0.1170 - accuracy: 0.9559\n",
      "Epoch 21/200\n",
      "82/82 [==============================] - 81s 992ms/step - loss: 0.1098 - accuracy: 0.9611\n",
      "Epoch 22/200\n",
      "82/82 [==============================] - 81s 984ms/step - loss: 0.0961 - accuracy: 0.9647\n",
      "Epoch 23/200\n",
      "82/82 [==============================] - 85s 1s/step - loss: 0.1054 - accuracy: 0.9607\n",
      "Epoch 24/200\n",
      "82/82 [==============================] - 86s 1s/step - loss: 0.1081 - accuracy: 0.9592\n",
      "Epoch 25/200\n",
      "82/82 [==============================] - 87s 1s/step - loss: 0.1017 - accuracy: 0.9634\n",
      "Epoch 26/200\n",
      "82/82 [==============================] - 87s 1s/step - loss: 0.1015 - accuracy: 0.9670\n",
      "Epoch 27/200\n",
      "82/82 [==============================] - 81s 986ms/step - loss: 0.1005 - accuracy: 0.9632\n",
      "Epoch 28/200\n",
      "82/82 [==============================] - 79s 968ms/step - loss: 0.0888 - accuracy: 0.9670\n",
      "Epoch 29/200\n",
      "82/82 [==============================] - 79s 962ms/step - loss: 0.0919 - accuracy: 0.9647\n",
      "Epoch 30/200\n",
      "82/82 [==============================] - 79s 966ms/step - loss: 0.0925 - accuracy: 0.9682\n",
      "Epoch 31/200\n",
      "82/82 [==============================] - 79s 969ms/step - loss: 0.0864 - accuracy: 0.9707\n",
      "Epoch 32/200\n",
      "82/82 [==============================] - 81s 990ms/step - loss: 0.0992 - accuracy: 0.9624\n",
      "Epoch 33/200\n",
      "82/82 [==============================] - 79s 964ms/step - loss: 0.0925 - accuracy: 0.9666\n",
      "Epoch 34/200\n",
      "82/82 [==============================] - 79s 969ms/step - loss: 0.0831 - accuracy: 0.9686\n",
      "Epoch 35/200\n",
      "82/82 [==============================] - 79s 968ms/step - loss: 0.0943 - accuracy: 0.9670\n",
      "Epoch 36/200\n",
      "82/82 [==============================] - 79s 963ms/step - loss: 0.0893 - accuracy: 0.9688\n",
      "Epoch 37/200\n",
      "82/82 [==============================] - 79s 964ms/step - loss: 0.0871 - accuracy: 0.9674\n",
      "Epoch 38/200\n",
      "82/82 [==============================] - 85s 1s/step - loss: 0.0819 - accuracy: 0.9705\n",
      "Epoch 39/200\n",
      "82/82 [==============================] - 79s 966ms/step - loss: 0.0901 - accuracy: 0.9699\n",
      "Epoch 40/200\n",
      "82/82 [==============================] - 86s 1s/step - loss: 0.0814 - accuracy: 0.9705\n",
      "Epoch 41/200\n",
      "82/82 [==============================] - 90s 1s/step - loss: 0.0866 - accuracy: 0.9689\n",
      "Epoch 42/200\n",
      "82/82 [==============================] - 92s 1s/step - loss: 0.0754 - accuracy: 0.9732\n",
      "Epoch 43/200\n",
      "82/82 [==============================] - 89s 1s/step - loss: 0.0807 - accuracy: 0.9739\n",
      "Epoch 44/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0758 - accuracy: 0.9707\n",
      "Epoch 45/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0824 - accuracy: 0.9703\n",
      "Epoch 46/200\n",
      "82/82 [==============================] - 81s 982ms/step - loss: 0.0730 - accuracy: 0.9734\n",
      "Epoch 47/200\n",
      "82/82 [==============================] - 80s 977ms/step - loss: 0.0677 - accuracy: 0.9764\n",
      "Epoch 48/200\n",
      "82/82 [==============================] - 80s 980ms/step - loss: 0.0719 - accuracy: 0.9734\n",
      "Epoch 49/200\n",
      "82/82 [==============================] - 80s 972ms/step - loss: 0.0760 - accuracy: 0.9686\n",
      "Epoch 50/200\n",
      "82/82 [==============================] - 81s 982ms/step - loss: 0.0652 - accuracy: 0.9760\n",
      "Epoch 51/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0736 - accuracy: 0.9762\n",
      "Epoch 52/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0754 - accuracy: 0.9732\n",
      "Epoch 53/200\n",
      "82/82 [==============================] - 81s 984ms/step - loss: 0.0708 - accuracy: 0.9758\n",
      "Epoch 54/200\n",
      "82/82 [==============================] - 80s 978ms/step - loss: 0.0718 - accuracy: 0.9745\n",
      "Epoch 55/200\n",
      "82/82 [==============================] - 82s 994ms/step - loss: 0.0634 - accuracy: 0.9770\n",
      "Epoch 56/200\n",
      "82/82 [==============================] - 82s 1s/step - loss: 0.0693 - accuracy: 0.9755\n",
      "Epoch 57/200\n",
      "82/82 [==============================] - 80s 976ms/step - loss: 0.0709 - accuracy: 0.9760\n",
      "Epoch 58/200\n",
      "82/82 [==============================] - 84s 1s/step - loss: 0.0659 - accuracy: 0.9758\n",
      "Epoch 59/200\n",
      "82/82 [==============================] - 79s 966ms/step - loss: 0.0693 - accuracy: 0.9785\n",
      "Epoch 60/200\n",
      "82/82 [==============================] - 79s 969ms/step - loss: 0.0626 - accuracy: 0.9760\n",
      "Epoch 61/200\n",
      "82/82 [==============================] - 81s 990ms/step - loss: 0.0609 - accuracy: 0.9780\n",
      "Epoch 62/200\n",
      "82/82 [==============================] - 81s 990ms/step - loss: 0.0740 - accuracy: 0.9714\n",
      "Epoch 63/200\n",
      "82/82 [==============================] - 81s 988ms/step - loss: 0.0669 - accuracy: 0.9778\n",
      "Epoch 64/200\n",
      "82/82 [==============================] - 80s 980ms/step - loss: 0.0558 - accuracy: 0.9785\n",
      "Epoch 65/200\n",
      "82/82 [==============================] - 81s 986ms/step - loss: 0.0646 - accuracy: 0.9760\n",
      "Epoch 66/200\n",
      "82/82 [==============================] - 81s 988ms/step - loss: 0.0695 - accuracy: 0.9766\n",
      "Epoch 67/200\n",
      "82/82 [==============================] - 80s 980ms/step - loss: 0.0596 - accuracy: 0.9780\n",
      "Epoch 68/200\n",
      "82/82 [==============================] - 86s 1s/step - loss: 0.0590 - accuracy: 0.9795\n",
      "Epoch 69/200\n",
      "82/82 [==============================] - 78s 949ms/step - loss: 0.0619 - accuracy: 0.9797\n",
      "Epoch 70/200\n",
      "82/82 [==============================] - 78s 951ms/step - loss: 0.0661 - accuracy: 0.9741\n",
      "Epoch 71/200\n",
      "82/82 [==============================] - 78s 946ms/step - loss: 0.0610 - accuracy: 0.9795\n",
      "Epoch 72/200\n",
      "82/82 [==============================] - 79s 959ms/step - loss: 0.0634 - accuracy: 0.9766\n",
      "Epoch 73/200\n",
      "82/82 [==============================] - 79s 958ms/step - loss: 0.0599 - accuracy: 0.9799\n",
      "Epoch 74/200\n",
      "82/82 [==============================] - 77s 937ms/step - loss: 0.0657 - accuracy: 0.9760\n",
      "Epoch 75/200\n",
      "82/82 [==============================] - 77s 939ms/step - loss: 0.0558 - accuracy: 0.9797\n",
      "Epoch 76/200\n",
      "82/82 [==============================] - 78s 946ms/step - loss: 0.0521 - accuracy: 0.9814\n",
      "Epoch 77/200\n",
      "82/82 [==============================] - 77s 940ms/step - loss: 0.0536 - accuracy: 0.9801\n",
      "Epoch 78/200\n",
      "82/82 [==============================] - 78s 947ms/step - loss: 0.0571 - accuracy: 0.9812\n",
      "Epoch 79/200\n",
      "82/82 [==============================] - 78s 946ms/step - loss: 0.0613 - accuracy: 0.9783\n",
      "Epoch 80/200\n",
      "82/82 [==============================] - 77s 944ms/step - loss: 0.0589 - accuracy: 0.9791\n",
      "Epoch 81/200\n",
      "82/82 [==============================] - 78s 951ms/step - loss: 0.0660 - accuracy: 0.9764\n",
      "Epoch 82/200\n",
      "82/82 [==============================] - 79s 961ms/step - loss: 0.0537 - accuracy: 0.9801\n",
      "Epoch 83/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0572 - accuracy: 0.9772\n",
      "Epoch 84/200\n",
      "82/82 [==============================] - 80s 978ms/step - loss: 0.0512 - accuracy: 0.9816\n",
      "Epoch 85/200\n",
      "82/82 [==============================] - 81s 982ms/step - loss: 0.0545 - accuracy: 0.9795\n",
      "Epoch 86/200\n",
      "82/82 [==============================] - 80s 981ms/step - loss: 0.0562 - accuracy: 0.9785\n",
      "Epoch 87/200\n",
      "82/82 [==============================] - 80s 971ms/step - loss: 0.0665 - accuracy: 0.9785\n",
      "Epoch 88/200\n",
      "82/82 [==============================] - 77s 941ms/step - loss: 0.0510 - accuracy: 0.9824\n",
      "Epoch 89/200\n",
      "82/82 [==============================] - 80s 975ms/step - loss: 0.0534 - accuracy: 0.9803\n",
      "Epoch 90/200\n",
      "82/82 [==============================] - 80s 974ms/step - loss: 0.0474 - accuracy: 0.9835\n",
      "Epoch 91/200\n",
      "82/82 [==============================] - 81s 984ms/step - loss: 0.0525 - accuracy: 0.9818\n",
      "Epoch 92/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0543 - accuracy: 0.9791\n",
      "Epoch 93/200\n",
      "82/82 [==============================] - 81s 994ms/step - loss: 0.0548 - accuracy: 0.9801\n",
      "Epoch 94/200\n",
      "82/82 [==============================] - 80s 980ms/step - loss: 0.0533 - accuracy: 0.9797\n",
      "Epoch 95/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0505 - accuracy: 0.9816\n",
      "Epoch 96/200\n",
      "82/82 [==============================] - 84s 1s/step - loss: 0.0544 - accuracy: 0.9818\n",
      "Epoch 97/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0479 - accuracy: 0.9820\n",
      "Epoch 98/200\n",
      "82/82 [==============================] - 80s 970ms/step - loss: 0.0449 - accuracy: 0.9847\n",
      "Epoch 99/200\n",
      "82/82 [==============================] - 79s 965ms/step - loss: 0.0484 - accuracy: 0.9812\n",
      "Epoch 100/200\n",
      "82/82 [==============================] - 80s 974ms/step - loss: 0.0527 - accuracy: 0.9806\n",
      "Epoch 101/200\n",
      "82/82 [==============================] - 79s 967ms/step - loss: 0.0553 - accuracy: 0.9810\n",
      "Epoch 102/200\n",
      "82/82 [==============================] - 82s 999ms/step - loss: 0.0437 - accuracy: 0.9841\n",
      "Epoch 103/200\n",
      "82/82 [==============================] - 81s 992ms/step - loss: 0.0539 - accuracy: 0.9776\n",
      "Epoch 104/200\n",
      "82/82 [==============================] - 81s 991ms/step - loss: 0.0528 - accuracy: 0.9797\n",
      "Epoch 105/200\n",
      "82/82 [==============================] - 80s 976ms/step - loss: 0.0449 - accuracy: 0.9841\n",
      "Epoch 106/200\n",
      "82/82 [==============================] - 77s 941ms/step - loss: 0.0489 - accuracy: 0.9810\n",
      "Epoch 107/200\n",
      "82/82 [==============================] - 79s 957ms/step - loss: 0.0447 - accuracy: 0.9837\n",
      "Epoch 108/200\n",
      "82/82 [==============================] - 78s 953ms/step - loss: 0.0482 - accuracy: 0.9833\n",
      "Epoch 109/200\n",
      "82/82 [==============================] - 79s 965ms/step - loss: 0.0401 - accuracy: 0.9845\n",
      "Epoch 110/200\n",
      "82/82 [==============================] - 80s 982ms/step - loss: 0.0470 - accuracy: 0.9822\n",
      "Epoch 111/200\n",
      "82/82 [==============================] - 77s 942ms/step - loss: 0.0428 - accuracy: 0.9835\n",
      "Epoch 112/200\n",
      "82/82 [==============================] - 77s 942ms/step - loss: 0.0481 - accuracy: 0.9816\n",
      "Epoch 113/200\n",
      "82/82 [==============================] - 82s 1s/step - loss: 0.0426 - accuracy: 0.9854\n",
      "Epoch 114/200\n",
      "82/82 [==============================] - 81s 991ms/step - loss: 0.0432 - accuracy: 0.9827\n",
      "Epoch 115/200\n",
      "82/82 [==============================] - 78s 950ms/step - loss: 0.0443 - accuracy: 0.9854\n",
      "Epoch 116/200\n",
      "82/82 [==============================] - 77s 943ms/step - loss: 0.0363 - accuracy: 0.9860\n",
      "Epoch 117/200\n",
      "82/82 [==============================] - 79s 959ms/step - loss: 0.0434 - accuracy: 0.9860\n",
      "Epoch 118/200\n",
      "82/82 [==============================] - 78s 949ms/step - loss: 0.0425 - accuracy: 0.9850\n",
      "Epoch 119/200\n",
      "82/82 [==============================] - 77s 941ms/step - loss: 0.0433 - accuracy: 0.9843\n",
      "Epoch 120/200\n",
      "82/82 [==============================] - 78s 945ms/step - loss: 0.0498 - accuracy: 0.9820\n",
      "Epoch 121/200\n",
      "82/82 [==============================] - 77s 941ms/step - loss: 0.0458 - accuracy: 0.9850\n",
      "Epoch 122/200\n",
      "82/82 [==============================] - 78s 946ms/step - loss: 0.0396 - accuracy: 0.9852\n",
      "Epoch 123/200\n",
      "82/82 [==============================] - 78s 956ms/step - loss: 0.0515 - accuracy: 0.9814\n",
      "Epoch 124/200\n",
      "82/82 [==============================] - 82s 996ms/step - loss: 0.0506 - accuracy: 0.9816\n",
      "Epoch 125/200\n",
      "82/82 [==============================] - 82s 1s/step - loss: 0.0377 - accuracy: 0.9870\n",
      "Epoch 126/200\n",
      "82/82 [==============================] - 84s 1s/step - loss: 0.0454 - accuracy: 0.9826\n",
      "Epoch 127/200\n",
      "82/82 [==============================] - 84s 1s/step - loss: 0.0452 - accuracy: 0.9839\n",
      "Epoch 128/200\n",
      "82/82 [==============================] - 84s 1s/step - loss: 0.0451 - accuracy: 0.9831\n",
      "Epoch 129/200\n",
      "82/82 [==============================] - 84s 1s/step - loss: 0.0326 - accuracy: 0.9875\n",
      "Epoch 130/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0455 - accuracy: 0.9831\n",
      "Epoch 131/200\n",
      "82/82 [==============================] - 82s 1s/step - loss: 0.0410 - accuracy: 0.9845\n",
      "Epoch 132/200\n",
      "82/82 [==============================] - 84s 1s/step - loss: 0.0447 - accuracy: 0.9835\n",
      "Epoch 133/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0335 - accuracy: 0.9879\n",
      "Epoch 134/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0366 - accuracy: 0.9856\n",
      "Epoch 135/200\n",
      "82/82 [==============================] - 80s 972ms/step - loss: 0.0353 - accuracy: 0.9891\n",
      "Epoch 136/200\n",
      "82/82 [==============================] - 77s 938ms/step - loss: 0.0330 - accuracy: 0.9881\n",
      "Epoch 137/200\n",
      "82/82 [==============================] - 77s 934ms/step - loss: 0.0403 - accuracy: 0.9864\n",
      "Epoch 138/200\n",
      "82/82 [==============================] - 76s 931ms/step - loss: 0.0364 - accuracy: 0.9870\n",
      "Epoch 139/200\n",
      "82/82 [==============================] - 76s 929ms/step - loss: 0.0367 - accuracy: 0.9866\n",
      "Epoch 140/200\n",
      "82/82 [==============================] - 77s 933ms/step - loss: 0.0423 - accuracy: 0.9843\n",
      "Epoch 141/200\n",
      "82/82 [==============================] - 77s 941ms/step - loss: 0.0439 - accuracy: 0.9837\n",
      "Epoch 142/200\n",
      "82/82 [==============================] - 77s 938ms/step - loss: 0.0408 - accuracy: 0.9843\n",
      "Epoch 143/200\n",
      "82/82 [==============================] - 79s 962ms/step - loss: 0.0356 - accuracy: 0.9875\n",
      "Epoch 144/200\n",
      "82/82 [==============================] - 80s 970ms/step - loss: 0.0407 - accuracy: 0.9858\n",
      "Epoch 145/200\n",
      "82/82 [==============================] - 79s 966ms/step - loss: 0.0379 - accuracy: 0.9866\n",
      "Epoch 146/200\n",
      "82/82 [==============================] - 79s 968ms/step - loss: 0.0299 - accuracy: 0.9900\n",
      "Epoch 147/200\n",
      "82/82 [==============================] - 79s 967ms/step - loss: 0.0458 - accuracy: 0.9849\n",
      "Epoch 148/200\n",
      "82/82 [==============================] - 78s 956ms/step - loss: 0.0258 - accuracy: 0.9893\n",
      "Epoch 149/200\n",
      "82/82 [==============================] - 79s 966ms/step - loss: 0.0417 - accuracy: 0.9854\n",
      "Epoch 150/200\n",
      "82/82 [==============================] - 79s 966ms/step - loss: 0.0316 - accuracy: 0.9881\n",
      "Epoch 151/200\n",
      "82/82 [==============================] - 78s 948ms/step - loss: 0.0330 - accuracy: 0.9893\n",
      "Epoch 152/200\n",
      "82/82 [==============================] - 84s 1s/step - loss: 0.0326 - accuracy: 0.9887\n",
      "Epoch 153/200\n",
      "82/82 [==============================] - 85s 1s/step - loss: 0.0334 - accuracy: 0.9868\n",
      "Epoch 154/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0433 - accuracy: 0.9833\n",
      "Epoch 155/200\n",
      "82/82 [==============================] - 81s 993ms/step - loss: 0.0331 - accuracy: 0.9866\n",
      "Epoch 156/200\n",
      "82/82 [==============================] - 80s 980ms/step - loss: 0.0397 - accuracy: 0.9839\n",
      "Epoch 157/200\n",
      "82/82 [==============================] - 80s 976ms/step - loss: 0.0303 - accuracy: 0.9895\n",
      "Epoch 158/200\n",
      "82/82 [==============================] - 79s 968ms/step - loss: 0.0362 - accuracy: 0.9868\n",
      "Epoch 159/200\n",
      "82/82 [==============================] - 80s 976ms/step - loss: 0.0328 - accuracy: 0.9870\n",
      "Epoch 160/200\n",
      "82/82 [==============================] - 80s 971ms/step - loss: 0.0318 - accuracy: 0.9873\n",
      "Epoch 161/200\n",
      "82/82 [==============================] - 79s 962ms/step - loss: 0.0349 - accuracy: 0.9881\n",
      "Epoch 162/200\n",
      "82/82 [==============================] - 79s 966ms/step - loss: 0.0309 - accuracy: 0.9908\n",
      "Epoch 163/200\n",
      "82/82 [==============================] - 80s 972ms/step - loss: 0.0271 - accuracy: 0.9916\n",
      "Epoch 164/200\n",
      "82/82 [==============================] - 79s 958ms/step - loss: 0.0386 - accuracy: 0.9875\n",
      "Epoch 165/200\n",
      "82/82 [==============================] - 89s 1s/step - loss: 0.0296 - accuracy: 0.9902\n",
      "Epoch 166/200\n",
      "82/82 [==============================] - 88s 1s/step - loss: 0.0385 - accuracy: 0.9866\n",
      "Epoch 167/200\n",
      "82/82 [==============================] - 90s 1s/step - loss: 0.0305 - accuracy: 0.9885\n",
      "Epoch 168/200\n",
      "82/82 [==============================] - 88s 1s/step - loss: 0.0252 - accuracy: 0.9904\n",
      "Epoch 169/200\n",
      "82/82 [==============================] - 90s 1s/step - loss: 0.0332 - accuracy: 0.9875\n",
      "Epoch 170/200\n",
      "82/82 [==============================] - 88s 1s/step - loss: 0.0389 - accuracy: 0.9872\n",
      "Epoch 171/200\n",
      "82/82 [==============================] - 89s 1s/step - loss: 0.0366 - accuracy: 0.9864\n",
      "Epoch 172/200\n",
      "82/82 [==============================] - 85s 1s/step - loss: 0.0344 - accuracy: 0.9868\n",
      "Epoch 173/200\n",
      "82/82 [==============================] - 87s 1s/step - loss: 0.0386 - accuracy: 0.9868\n",
      "Epoch 174/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0345 - accuracy: 0.9877\n",
      "Epoch 175/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0271 - accuracy: 0.9902\n",
      "Epoch 176/200\n",
      "82/82 [==============================] - 87s 1s/step - loss: 0.0437 - accuracy: 0.9852\n",
      "Epoch 177/200\n",
      "82/82 [==============================] - 88s 1s/step - loss: 0.0411 - accuracy: 0.9852\n",
      "Epoch 178/200\n",
      "82/82 [==============================] - 88s 1s/step - loss: 0.0230 - accuracy: 0.9906\n",
      "Epoch 179/200\n",
      "82/82 [==============================] - 88s 1s/step - loss: 0.0262 - accuracy: 0.9891\n",
      "Epoch 180/200\n",
      "82/82 [==============================] - 87s 1s/step - loss: 0.0251 - accuracy: 0.9906\n",
      "Epoch 181/200\n",
      "82/82 [==============================] - 89s 1s/step - loss: 0.0262 - accuracy: 0.9906\n",
      "Epoch 182/200\n",
      "82/82 [==============================] - 95s 1s/step - loss: 0.0364 - accuracy: 0.9860\n",
      "Epoch 183/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0384 - accuracy: 0.9852\n",
      "Epoch 184/200\n",
      "82/82 [==============================] - 92s 1s/step - loss: 0.0236 - accuracy: 0.9921\n",
      "Epoch 185/200\n",
      "82/82 [==============================] - 103s 1s/step - loss: 0.0301 - accuracy: 0.9883\n",
      "Epoch 186/200\n",
      "82/82 [==============================] - 94s 1s/step - loss: 0.0267 - accuracy: 0.9902\n",
      "Epoch 187/200\n",
      "82/82 [==============================] - 83s 1s/step - loss: 0.0366 - accuracy: 0.9872\n",
      "Epoch 188/200\n",
      "82/82 [==============================] - 81s 990ms/step - loss: 0.0255 - accuracy: 0.9914\n",
      "Epoch 189/200\n",
      "82/82 [==============================] - 81s 986ms/step - loss: 0.0272 - accuracy: 0.9891\n",
      "Epoch 190/200\n",
      "82/82 [==============================] - 81s 988ms/step - loss: 0.0281 - accuracy: 0.9904\n",
      "Epoch 191/200\n",
      "82/82 [==============================] - 80s 976ms/step - loss: 0.0266 - accuracy: 0.9904\n",
      "Epoch 192/200\n",
      "82/82 [==============================] - 81s 982ms/step - loss: 0.0268 - accuracy: 0.9891\n",
      "Epoch 193/200\n",
      "82/82 [==============================] - 79s 963ms/step - loss: 0.0294 - accuracy: 0.9887\n",
      "Epoch 194/200\n",
      "82/82 [==============================] - 80s 978ms/step - loss: 0.0290 - accuracy: 0.9895\n",
      "Epoch 195/200\n",
      "82/82 [==============================] - 80s 972ms/step - loss: 0.0256 - accuracy: 0.9902\n",
      "Epoch 196/200\n",
      "82/82 [==============================] - 80s 974ms/step - loss: 0.0293 - accuracy: 0.9902\n",
      "Epoch 197/200\n",
      "82/82 [==============================] - 79s 967ms/step - loss: 0.0301 - accuracy: 0.9877\n",
      "Epoch 198/200\n",
      "82/82 [==============================] - 79s 962ms/step - loss: 0.0270 - accuracy: 0.9900\n",
      "Epoch 199/200\n",
      "82/82 [==============================] - 72s 881ms/step - loss: 0.0292 - accuracy: 0.9902\n",
      "Epoch 200/200\n",
      "82/82 [==============================] - 68s 832ms/step - loss: 0.0287 - accuracy: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x228d8d9a280>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the CNN to the image\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "trainig_set = train_datagen.flow_from_directory(\n",
    "        r'D:\\chest_xray\\chest_xray\\train',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=64,\n",
    "        class_mode='binary')\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        r'D:\\chest_xray\\chest_xray\\test',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=64,\n",
    "        class_mode='binary')\n",
    "model.fit(\n",
    "        trainig_set,\n",
    "        steps_per_epoch=82,\n",
    "        epochs=200,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 11s 1s/step - loss: 0.5503 - accuracy: 0.9231\n",
      "Loss of the model: 0.55\n",
      "Test Accuracy: 92.31%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_set)\n",
    "\n",
    "print(\"Loss of the model: %.2f\"%(scores[0]))\n",
    "print(\"Test Accuracy: %.2f%%\"%(scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(filename):\n",
    "    img = load_img(filename, target_size=(64, 64))\n",
    "    image = keras.preprocessing.image.img_to_array(img)\n",
    "    image = image / 255.0\n",
    "    image = image.reshape(1,64,64,3)\n",
    "    model = tf.keras.models.load_model('custom_model.h5py')\n",
    "    prediction = model.predict(image)\n",
    "    print(prediction)\n",
    "    plt.imshow(img)\n",
    "    if(prediction[0] > 0.5):\n",
    "        stat = prediction[0] * 100 \n",
    "        print(\"This image is %.2f percent %s\"% (stat, \"PNEUMONIA\"))\n",
    "    else:\n",
    "        stat = (1.0 - prediction[0]) * 100\n",
    "        print(\"This image is %.2f percent %s\" % (stat, \"NORMAL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000228E040ED30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0.98445165]]\n",
      "This image is 98.45 percent PNEUMONIA\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_image(r\"D:\\chest_xray\\chest_xray\\test\\PNEUMONIA\\person15_virus_46.jpeg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
